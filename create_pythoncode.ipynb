{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b89d240-6da6-4c2a-9cd4-c52df329fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "from cpc_transformer import *\n",
    "from cpc_tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f10cee-1571-474d-92b7-293dfa0ac1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = 'F:/$daten/datasets/pythoncode/train/'\n",
    "with open(path+'collection.dat', 'rb') as f:\n",
    "    collection = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfb2209-429e-4a09-a9a8-686d76c5d91b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eec97797-6891-4407-b7cf-e94585c5422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 280 # maximale Anzahl Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9936035c-1de5-460f-a0d1-12cb1e1faaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`om 2016 & 2017 for the test set.  \n",
      "<M>\n",
      "Remove rows for which target column is em\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode(string, rnd=True):\n",
    "    'Tokenizes and encodes a string to a format suitable for the neural network'\n",
    "    if len(string) > MAX_LEN+1 and rnd:\n",
    "        i = np.random.randint(0, len(string)-MAX_LEN-1)\n",
    "        string = string[i:i+MAX_LEN]\n",
    "    string = '`' + string\n",
    "    string = tokenizer.tokenize(string)\n",
    "    x = np.zeros(MAX_LEN+1, dtype=int)\n",
    "    x[:] = tokenizer.c2t['´']\n",
    "    for k, l in enumerate(string[:MAX_LEN+1]):\n",
    "        x[k] = l\n",
    "    x = x[:MAX_LEN+1]\n",
    "    return x\n",
    "\n",
    "\n",
    "def onehot(ys):\n",
    "    bsize, maxlen = ys.shape\n",
    "    yt = np.zeros((bsize, maxlen, tokenizer.NTOK))\n",
    "    for i in range(bsize):\n",
    "        for j in range(maxlen):\n",
    "            yt[i,j,ys[i,j]] = 1.\n",
    "    return yt\n",
    "\n",
    "\n",
    "def batchgen(bsize=16):\n",
    "    ep = 0\n",
    "    while True:\n",
    "        inds = np.random.permutation(range(100, len(collection)))\n",
    "        minibatches = [ inds[k*bsize:(k+1)*bsize] for k in range(len(inds)//bsize) ]\n",
    "        for mb in minibatches:\n",
    "            xs = np.zeros((bsize, MAX_LEN+1), dtype=int)\n",
    "            for i, j in enumerate(mb):\n",
    "                x = collection[j]\n",
    "                xs[i] = encode(x)\n",
    "            ohs = onehot(xs)\n",
    "            ohs = ohs * 0.9 + np.ones_like(ohs)/tokenizer.NTOK * 0.1\n",
    "            yield xs, ohs\n",
    "        print(f'========== EPOCH {ep} COMPLETED ==========')\n",
    "        ep += 1\n",
    "    \n",
    "bg = batchgen()\n",
    "xs, oh = next(bg)\n",
    "print(tokenizer.detokenize(xs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d180306-6b16-4e4d-a8f4-f6e2d998de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n=128, nh=4, ntok=tokenizer.NTOK)\n",
    "from torch_optimizer import Lookahead, Yogi\n",
    "net.optim = Lookahead(Yogi(net.parameters(), lr=1e-3, betas=(0.9, 0.99)))\n",
    "net.iters = 0\n",
    "net.losses = []\n",
    "net.vlosses = []\n",
    "net.vmin = 9999\n",
    "bg = batchgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92d64fb8-2de5-4c38-a322-392ac716a11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['enc5.mha.q.weight', 'enc5.mha.q.bias', 'enc5.mha.k.weight', 'enc5.mha.k.bias', 'enc5.mha.v.weight', 'enc5.mha.v.bias', 'enc5.mha.p.weight', 'enc5.mha.p.bias', 'enc5.ln.gamma', 'enc5.ln.beta', 'enc5.ff.dense1.weight', 'enc5.ff.dense1.bias', 'enc5.ff.dense2.weight', 'enc5.ff.dense2.bias', 'enc5.ff.ln.gamma', 'enc5.ff.ln.beta', 'enc6.mha.q.weight', 'enc6.mha.q.bias', 'enc6.mha.k.weight', 'enc6.mha.k.bias', 'enc6.mha.v.weight', 'enc6.mha.v.bias', 'enc6.mha.p.weight', 'enc6.mha.p.bias', 'enc6.ln.gamma', 'enc6.ln.beta', 'enc6.ff.dense1.weight', 'enc6.ff.dense1.bias', 'enc6.ff.dense2.weight', 'enc6.ff.dense2.bias', 'enc6.ff.ln.gamma', 'enc6.ff.ln.beta'], unexpected_keys=[])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net.load_state_dict(torch.load('cpc_weights_231223.dat'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43fd2f15-92f8-4599-859c-0d2a7ef20d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.7206, device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor(5.7204, device='cuda:0', grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def valloss():\n",
    "    net.eval()\n",
    "    bsize = 64\n",
    "    xs = np.zeros((bsize, MAX_LEN+1), dtype=int)\n",
    "    for i in range(bsize):\n",
    "        x = collection[i]\n",
    "        xs[i] = encode(x)\n",
    "    ohs = onehot(xs)\n",
    "    ohs = ohs * 0.9 + np.ones_like(ohs)/tokenizer.NTOK * 0.1\n",
    "    xs, ohs = np2t(xs, ohs)\n",
    "    xp = net(xs.long())\n",
    "    xp = rearrange(xp[:,:-1], 'b p n -> (b p) n')\n",
    "    ohs = rearrange(ohs[:,1:], 'b p n -> (b p) n')\n",
    "    return torch.mean(-torch.log_softmax(xp, dim=1) * ohs) * tokenizer.NTOK\n",
    "        \n",
    "\n",
    "def loss():\n",
    "    net.train()\n",
    "    xs, ohs = next(bg)\n",
    "    xs, ohs = np2t(xs, ohs)\n",
    "    xp = net(xs.long())\n",
    "    xp = rearrange(xp[:,:-1], 'b p n -> (b p) n')\n",
    "    ohs = rearrange(ohs[:,1:], 'b p n -> (b p) n')\n",
    "    return torch.mean(-torch.log_softmax(xp, dim=1) * ohs) * tokenizer.NTOK\n",
    "\n",
    "valloss(), \\\n",
    "loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3ec07-e593-49de-b215-68ed5d1e2523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "vlosses = []\n",
    "\n",
    "for k in trange(99999999):\n",
    "    l = loss()\n",
    "    l.backward()\n",
    "    losses.append(l.item())\n",
    "    net.optim.step()\n",
    "    net.optim.zero_grad()\n",
    "    \n",
    "    if len(losses) == 100:\n",
    "        vloss = valloss().item()\n",
    "        if vloss < net.vmin:\n",
    "            net.vmin = vloss\n",
    "            torch.save(net.state_dict(), 'cpc_weights_261223.dat')\n",
    "        net.losses.append((net.iters, np.mean(losses)))\n",
    "        net.vlosses.append((net.iters, vloss))\n",
    "        losses = []\n",
    "    net.iters += 1\n",
    "    \n",
    "    if k % 100 == 0:\n",
    "        plt.plot(*zip(*net.losses), zorder=+20)\n",
    "        plt.plot(*zip(*net.vlosses))\n",
    "        #plt.ylim([0,50])\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        s, p = predict()\n",
    "        print('------')\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338185e-46a5-48a4-b330-3c5d4b9d9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(*zip(*net.losses), zorder=+20)\n",
    "plt.plot(*zip(*net.vlosses))\n",
    "#plt.ylim([0,50])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b591eefd-2f96-4df8-b514-6d82928abbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpe\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(string=None, tau=0, length=MAX_LEN):\n",
    "    if string is None:\n",
    "        i = np.random.randint(100)\n",
    "        x = collection[i]\n",
    "        x = encode(x)\n",
    "    else:\n",
    "        x = encode(string, rnd=False)\n",
    "        string = '`' + string\n",
    "        string = tokenizer.tokenize(string)\n",
    "        x = np.zeros(len(string), dtype=int)\n",
    "        for k, l in enumerate(string):\n",
    "            x[k] = l\n",
    "    \n",
    "    ys = np2t([x]).long()\n",
    "    print(tokenizer.detokenize(x))\n",
    "\n",
    "    net.eval()\n",
    "    probs = []\n",
    "    for i in range(0, length):\n",
    "        yp = net(ys)\n",
    "        dist = t2np(F.softmax(yp[0,-1], dim=0))\n",
    "        if tau>0:\n",
    "            k = np.random.choice(range(tokenizer.NTOK), p=t2np(F.softmax(yp[0,-1]/tau, dim=0))) \n",
    "        else:\n",
    "            k = dist.argmax()\n",
    "        ys = torch.cat([ys, np2t([[k]])], dim=1).long()\n",
    "        ys = ys[:,-MAX_LEN:]\n",
    "        p = dist[k]\n",
    "        probs.append(p)\n",
    "        if k == tokenizer.c2t['´']:\n",
    "            break\n",
    "\n",
    "    p = np.prod(probs)\n",
    "\n",
    "    ys = t2np(ys[0, 0:i])\n",
    "    s = tokenizer.detokenize(ys)\n",
    "\n",
    "    return s, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9908ebc0-9dd4-4252-be14-e050f2df72f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`\n",
      "net.\n",
      "`\n",
      "net.reset_index(drop=True, inplace=True)\n",
      "net.reset_index(drop=True, inplace=T\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"\n",
    "net.\"\"\"\n",
    "print(predict(string, tau=0.5, length=80)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b5d9e9a-fadf-41ca-980e-95b6bd7dd8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`>\n",
      "import numpy as np\n",
      "<C>\n",
      "import matplotlib.pyplot as plt\n",
      "test_df=pd.read_csv(\"../input/testPrice.csv\")\n",
      "<C>\n",
      "cols_to_use = [\"supply_area\", \"exclusive_use_area\", \"floor\", \"room_count\", \"bathroom_count\",\"total_household_count_in_sites\",\"total_parking_capacity_in_site\",\"apartment_buil\n",
      "d_capacity_in_site\",\"total_parking_capacity_in_site\",\"total_parking_capacity_in_site\",\"total_parking_capacity_in_site\",\"total_parking_capacity_in_site\",\"total_parking_capacity_in_site\",\"total_parking_capacity_in_site\",\"total_parking_capacity_in_site\",\"total_parking_capacity_in_s\n"
     ]
    }
   ],
   "source": [
    "print(predict(tau=0.5)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
